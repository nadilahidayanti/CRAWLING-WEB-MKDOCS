{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Nama : NADILA HIDAYANTI NIM : 160411100182 SEMESTER : 6 MATA KULIAH : PENCARIAN DAN PENAMBANGAN WEB Library yang dibutuhkan untuk me-crawling data di web Sastrawi BeautifulSoup4 Sklearn Numpy Request from math import log10 from requests import get from bs4 import BeautifulSoup import sqlite3 from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from Sastrawi.Stemmer.StemmerFactory import StemmerFactory import csv from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import numpy as np import warnings warnings . filterwarnings ( ignore ) Penjelasan Fungsi dari Library Sastrawi Steeming bertujuan untuk mentransofrmasikan kata menjadi kata dasarnya (root) dengan menghilangkan semua imbuhan kata (affixes) meliputi awalan kata (prefixes), sisipin kata (infixes), akhiran kata (suffixes) dan atau menghilangkan awalan dan akhiran kata (confixes) pada kata turunan begitulah fungsi dari Library Sastrawi yang digunakan untuk mengambil kata dasar dan membuang kata yang tidak penting BeautifulSoup4 BeautifulSoup merupakan library yang terdapat di python untuk menarik data yang ada di HTML. ada banyak library yang dapat digunakan untuk menarik data pada web namun saya kali ini menggunakan BeautifulSoup. Sklearn Penyedia rumus yang digunakan untuk menghitung nilai K-Means dan Shilhouette Numpy Untuk mengolah data numerrik dan digunakan untuk operasi vektor dan matriks Request Digunakan saat me-Crawl data pada web . Note : Cara menjalankan library tersebut bisa menggunakan CMD disesuaikan dengan versi Idle Python nya atau jika tidak mendownload library satu persatu bisa menggunakan Aplikasi Spyder versi disesuaikan dengan laptop pengguna. Referensi https://devtrik.com/python/steeming-bahasa-indonesia-python-sastrawi/ https://datascience.or.id/article/Mengenal-BeautifulSoup-Untuk-WebScraping-5a8fa6e6 https://www.codepolitan.com/5-library-python-untuk-data-science-59b774b6cad97","title":"Pengantar"},{"location":"#nama-nadila-hidayanti","text":"","title":"Nama : NADILA HIDAYANTI"},{"location":"#nim-160411100182","text":"","title":"NIM : 160411100182"},{"location":"#semester-6","text":"","title":"SEMESTER : 6"},{"location":"#mata-kuliah-pencarian-dan-penambangan-web","text":"","title":"MATA KULIAH : PENCARIAN DAN PENAMBANGAN WEB"},{"location":"#library-yang-dibutuhkan-untuk-me-crawling-data-di-web","text":"Sastrawi BeautifulSoup4 Sklearn Numpy Request from math import log10 from requests import get from bs4 import BeautifulSoup import sqlite3 from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from Sastrawi.Stemmer.StemmerFactory import StemmerFactory import csv from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import numpy as np import warnings warnings . filterwarnings ( ignore )","title":"Library yang dibutuhkan untuk me-crawling data di web"},{"location":"#penjelasan-fungsi-dari-library","text":"Sastrawi Steeming bertujuan untuk mentransofrmasikan kata menjadi kata dasarnya (root) dengan menghilangkan semua imbuhan kata (affixes) meliputi awalan kata (prefixes), sisipin kata (infixes), akhiran kata (suffixes) dan atau menghilangkan awalan dan akhiran kata (confixes) pada kata turunan begitulah fungsi dari Library Sastrawi yang digunakan untuk mengambil kata dasar dan membuang kata yang tidak penting BeautifulSoup4 BeautifulSoup merupakan library yang terdapat di python untuk menarik data yang ada di HTML. ada banyak library yang dapat digunakan untuk menarik data pada web namun saya kali ini menggunakan BeautifulSoup. Sklearn Penyedia rumus yang digunakan untuk menghitung nilai K-Means dan Shilhouette Numpy Untuk mengolah data numerrik dan digunakan untuk operasi vektor dan matriks Request Digunakan saat me-Crawl data pada web .","title":"Penjelasan Fungsi dari Library"},{"location":"#note","text":"Cara menjalankan library tersebut bisa menggunakan CMD disesuaikan dengan versi Idle Python nya atau jika tidak mendownload library satu persatu bisa menggunakan Aplikasi Spyder versi disesuaikan dengan laptop pengguna.","title":"Note :"},{"location":"#referensi","text":"https://devtrik.com/python/steeming-bahasa-indonesia-python-sastrawi/ https://datascience.or.id/article/Mengenal-BeautifulSoup-Untuk-WebScraping-5a8fa6e6 https://www.codepolitan.com/5-library-python-untuk-data-science-59b774b6cad97","title":"Referensi"},{"location":"Crawling/","text":"Crawl Crawling disini yang dimaksud adalah di gunakan untuk mengambil data yang berada dalam suatu website yang berupa teks, audio, foto dan video. Code dan Penjelasan untuk me-Crawling data pada Website Review Buku (Lensa Buku) src = http://www.lensabuku.com/page/ for cinta in range ( 30 ): print ( cinta ) page = get ( src + %s % cinta ) soup = BeautifulSoup ( page . content , html.parser ) link = soup . findAll ( class_ = more-link ) komen = soup . findAll ( class_ = entry-comments-link ) #Digunakan untuk mengambil page yang ada dalam link tersebut hingga page 30 for l in range ( len ( link )): page = get ( link [ l ][ href ]) soup = BeautifulSoup ( page . content , html.parser ) konten = soup . find ( class_ = entry-content ) judul = soup . find ( class_ = entry-title ) . getText () komentar = komen [ l ] . getText () tulisan = konten . findAll ( p ) isi = for i in tulisan : isi += i . getText () conn . execute ( INSERT INTO BUKU(JUDUL,ISI,KOMENTAR) values (?, ?, ?) , ( judul , isi , komentar )) #untuk mengambil data berdasarkan class conn . commit () cursor = conn . execute ( SELECT* from BUKU ) isi = [] for row in cursor : isi . append ( row [ 1 ]) print ( row ) #Untuk menyimpan hasil crawl ke dalam database sqlite Setiap website cara me-Crawl nya berbeda. code diatas hanya me-Crawl judul dan isi review buku hingga page 30 . Setelah hasil running program selesai , hasil di simpan dalam bentuk database Sqlite dan CSV . Hasil Running Program : ;","title":"Crawling"},{"location":"Crawling/#crawl","text":"Crawling disini yang dimaksud adalah di gunakan untuk mengambil data yang berada dalam suatu website yang berupa teks, audio, foto dan video.","title":"Crawl"},{"location":"Crawling/#code-dan-penjelasan-untuk-me-crawling-data-pada-website-review-buku-lensa-buku","text":"src = http://www.lensabuku.com/page/ for cinta in range ( 30 ): print ( cinta ) page = get ( src + %s % cinta ) soup = BeautifulSoup ( page . content , html.parser ) link = soup . findAll ( class_ = more-link ) komen = soup . findAll ( class_ = entry-comments-link ) #Digunakan untuk mengambil page yang ada dalam link tersebut hingga page 30 for l in range ( len ( link )): page = get ( link [ l ][ href ]) soup = BeautifulSoup ( page . content , html.parser ) konten = soup . find ( class_ = entry-content ) judul = soup . find ( class_ = entry-title ) . getText () komentar = komen [ l ] . getText () tulisan = konten . findAll ( p ) isi = for i in tulisan : isi += i . getText () conn . execute ( INSERT INTO BUKU(JUDUL,ISI,KOMENTAR) values (?, ?, ?) , ( judul , isi , komentar )) #untuk mengambil data berdasarkan class conn . commit () cursor = conn . execute ( SELECT* from BUKU ) isi = [] for row in cursor : isi . append ( row [ 1 ]) print ( row ) #Untuk menyimpan hasil crawl ke dalam database sqlite Setiap website cara me-Crawl nya berbeda. code diatas hanya me-Crawl judul dan isi review buku hingga page 30 . Setelah hasil running program selesai , hasil di simpan dalam bentuk database Sqlite dan CSV .","title":"Code dan Penjelasan untuk me-Crawling data pada Website Review Buku (Lensa Buku)"},{"location":"Crawling/#hasil-running-program","text":";","title":"Hasil Running Program :"},{"location":"Graph/","text":"Graph Graph adalah kumpulan dati titik (node) dan garis dimana pasangan \u2013 pasangan titik (node) tersebut dihubungkan oleh segmen garis. Node ini biasa disebut simpul (vertex) dan segmen garis disebut ruas (edge) Code g = nx . from_pandas_edgelist ( edgelistFrame , From , To , None , nx . DiGraph ()) pos = nx . spring_layout ( g ) nx . draw ( g , pos ) nx . draw_networkx_labels ( g , pos , label , font_color = w ) plt . axis ( off ) plt . show () Penjelasan Ketika tahap crawling sebelumnya sudah selesai maka telah memiliki data yang telah tersimpan pada list yang kemudian akan diproses untuk dijadikan sebuah graph Pada proses graph ini menggunakan library Network as nx kemudian lanjut diproses menggunakan matplotlib.pyplot as plt agar graph mudah dipahami. Hasil Run Program","title":"Graph"},{"location":"Graph/#graph","text":"Graph adalah kumpulan dati titik (node) dan garis dimana pasangan \u2013 pasangan titik (node) tersebut dihubungkan oleh segmen garis. Node ini biasa disebut simpul (vertex) dan segmen garis disebut ruas (edge)","title":"Graph"},{"location":"Graph/#code","text":"g = nx . from_pandas_edgelist ( edgelistFrame , From , To , None , nx . DiGraph ()) pos = nx . spring_layout ( g ) nx . draw ( g , pos ) nx . draw_networkx_labels ( g , pos , label , font_color = w ) plt . axis ( off ) plt . show ()","title":"Code"},{"location":"Graph/#penjelasan","text":"Ketika tahap crawling sebelumnya sudah selesai maka telah memiliki data yang telah tersimpan pada list yang kemudian akan diproses untuk dijadikan sebuah graph Pada proses graph ini menggunakan library Network as nx kemudian lanjut diproses menggunakan matplotlib.pyplot as plt agar graph mudah dipahami.","title":"Penjelasan"},{"location":"Graph/#hasil-run-program","text":"","title":"Hasil Run Program"},{"location":"K-Means/","text":"K-Maens K-Maens suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi. Code dan Penjelasan kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ #digunakan untuk me-cluster data yang sudah diproses sebelumnya Referensi https://yudiagusta.wordpress.com/k-means/ http://madhugnadig.com/articles/machine-learning/2017/03/04/implementing-k-means-clustering-from-scratch-in-python.html Hasil Running","title":"K-Means"},{"location":"K-Means/#k-maens","text":"K-Maens suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi.","title":"K-Maens"},{"location":"K-Means/#code-dan-penjelasan","text":"kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ #digunakan untuk me-cluster data yang sudah diproses sebelumnya","title":"Code dan Penjelasan"},{"location":"K-Means/#referensi","text":"https://yudiagusta.wordpress.com/k-means/ http://madhugnadig.com/articles/machine-learning/2017/03/04/implementing-k-means-clustering-from-scratch-in-python.html","title":"Referensi"},{"location":"K-Means/#hasil-running","text":"","title":"Hasil Running"},{"location":"PageRank/","text":"PageRank PageRank adalah nilai yang dimiliki oleh suatu node. Semakin tinggi nilai PageRank, semakin \u201cpenting dan relevan\u201d node tersebut,sebuah algoritme yang telah dipatenkan yang berfungsi menentukan situs web mana yang lebih penting/populer. Code damping = 0.85 max_iterr = 100 error_toleransi = 0.0001 pr = nx . pagerank ( g , alpha = damping , max_iter = max_iterr , tol = error_toleransi ) print ( keterangan node: ) nodelist = g . nodes label = {} data = [] for i , key in enumerate ( nodelist ): data . append (( pr [ key ], key )) label [ key ] = i #print(i, key, pr[key]) Penjelasan Code diatas untuk menentukan hasil pagerank dari tertinggi hingga terkecil yang didapat dari proses crawling sebelumnya yang telah ditampung didalam list. urut = data . copy () for x in range ( len ( urut )): for y in range ( len ( urut )): if urut [ x ][ 0 ] urut [ y ][ 0 ]: urut [ x ], urut [ y ] = urut [ y ], urut [ x ] urut = pd . DataFrame ( urut , None , ( PageRank , Node )) print ( urut ) Code diatas untuk mengurutkan nilai Pagerank dari terbesar hingga terkecil. Hasil Running Program","title":"Page Rank"},{"location":"PageRank/#pagerank","text":"PageRank adalah nilai yang dimiliki oleh suatu node. Semakin tinggi nilai PageRank, semakin \u201cpenting dan relevan\u201d node tersebut,sebuah algoritme yang telah dipatenkan yang berfungsi menentukan situs web mana yang lebih penting/populer.","title":"PageRank"},{"location":"PageRank/#code","text":"damping = 0.85 max_iterr = 100 error_toleransi = 0.0001 pr = nx . pagerank ( g , alpha = damping , max_iter = max_iterr , tol = error_toleransi ) print ( keterangan node: ) nodelist = g . nodes label = {} data = [] for i , key in enumerate ( nodelist ): data . append (( pr [ key ], key )) label [ key ] = i #print(i, key, pr[key])","title":"Code"},{"location":"PageRank/#penjelasan","text":"Code diatas untuk menentukan hasil pagerank dari tertinggi hingga terkecil yang didapat dari proses crawling sebelumnya yang telah ditampung didalam list. urut = data . copy () for x in range ( len ( urut )): for y in range ( len ( urut )): if urut [ x ][ 0 ] urut [ y ][ 0 ]: urut [ x ], urut [ y ] = urut [ y ], urut [ x ] urut = pd . DataFrame ( urut , None , ( PageRank , Node )) print ( urut ) Code diatas untuk mengurutkan nilai Pagerank dari terbesar hingga terkecil.","title":"Penjelasan"},{"location":"PageRank/#hasil-running-program","text":"","title":"Hasil Running Program"},{"location":"Seleksi_Fitur/","text":"Seleksi Fitur Code dan Penjelasan Seleksi fitur adalah menentukan istilah-istilah yang layak dijadikan term index atau dengan kata lain membuang (menghilangkan) istilah-istilah yang tidak mungkin dijadikan indeks. Terdapat beberapa cara yang dapat dilakukan untuk mengeliminasi istilah-istilah yang kurang merepresentasikan dokumen tersebut, diantaranya adalah menghilangkan istilah-istilah yang sering muncul pada berbagai dokumen . Istilah-istilah yang sering muncul pada berbagai dokumen biasanya adalah istilah-istilah yang tidak mempunyai arti terhadap dokumen tersebut, jika istilah ini dihilangkan, tidak mengurangi makna dokumennya. def pearsonCalculate ( data , u , v ): i, j is an index atas = 0 ; bawah_kiri = 0 ; bawah_kanan = 0 for k in range ( len ( data )): atas += ( data [ k , u ] - meanFitur [ u ]) * ( data [ k , v ] - meanFitur [ v ]) bawah_kiri += ( data [ k , u ] - meanFitur [ u ]) ** 2 bawah_kanan += ( data [ k , v ] - meanFitur [ v ]) ** 2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas / ( bawah_kiri * bawah_kanan ) #untuk menghitung rumus person def meanF ( data ): meanFitur = [] for i in range ( len ( data [ 0 ])): meanFitur . append ( sum ( data [:, i ]) / len ( data )) return np . array ( meanFitur ) #untuk mencari rata rata per fitur def seleksiFiturPearson ( katadasar , data , threshold ): global meanFitur meanFitur = meanF ( data ) u = 0 while u len ( data [ 0 ]): dataBaru = data [:, : u + 1 ] meanBaru = meanFitur [: u + 1 ] katadasarBaru = katadasar [: u + 1 ] v = u while v len ( data [ 0 ]): if u != v : value = pearsonCalculate ( data , u , v ) if value threshold : dataBaru = np . hstack (( dataBaru , data [:, v ] . reshape ( data . shape [ 0 ], 1 ))) meanBaru = np . hstack (( meanBaru , meanFitur [ v ])) katadasarBaru = np . hstack (( katadasarBaru , katadasar [ v ])) v += 1 data = dataBaru meanFitur = meanBaru katadasar = katadasarBaru if u % 50 == 0 : print ( proses : , u , data . shape ) u += 1 return katadasar , data #untuk menseleksi fitur katadasarBaru , fiturBaru = seleksiFiturPearson ( katadasar , tfidf , 0.8 ) #untuk memanggil function for i in range ( 2 , len ( fiturBaru ) - 1 ): kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ s_avg = silhouette_score ( fiturBaru , classnya , random_state = 0 ) print ( Silhouette untuk , i , cluster adalah , s_avg ) print ( kmeans . labels_ ) #untuk mencari kmeans dan silhoutte print ( proses selesai ) with open ( Anggota_cluster.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) for i in classnya . reshape ( - 1 , 1 ): employee_writer . writerow ( i ) with open ( Seleksi_Fitur.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ([ katadasarBaru . tolist ()]) for i in fiturBaru : employee_writer . writerow ( i ) # untuk menyimpan data ke CSV Referensi https://wahyudisetiawan.wordpress.com/tag/seleksi-fitur/ http://www.softscients.web.id/2019/03/feature-selection-untuk-meningkatkan.html Hasil Running","title":"Seleksi Fitur"},{"location":"Seleksi_Fitur/#seleksi-fitur","text":"","title":"Seleksi Fitur"},{"location":"Seleksi_Fitur/#code-dan-penjelasan","text":"Seleksi fitur adalah menentukan istilah-istilah yang layak dijadikan term index atau dengan kata lain membuang (menghilangkan) istilah-istilah yang tidak mungkin dijadikan indeks. Terdapat beberapa cara yang dapat dilakukan untuk mengeliminasi istilah-istilah yang kurang merepresentasikan dokumen tersebut, diantaranya adalah menghilangkan istilah-istilah yang sering muncul pada berbagai dokumen . Istilah-istilah yang sering muncul pada berbagai dokumen biasanya adalah istilah-istilah yang tidak mempunyai arti terhadap dokumen tersebut, jika istilah ini dihilangkan, tidak mengurangi makna dokumennya. def pearsonCalculate ( data , u , v ): i, j is an index atas = 0 ; bawah_kiri = 0 ; bawah_kanan = 0 for k in range ( len ( data )): atas += ( data [ k , u ] - meanFitur [ u ]) * ( data [ k , v ] - meanFitur [ v ]) bawah_kiri += ( data [ k , u ] - meanFitur [ u ]) ** 2 bawah_kanan += ( data [ k , v ] - meanFitur [ v ]) ** 2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas / ( bawah_kiri * bawah_kanan ) #untuk menghitung rumus person def meanF ( data ): meanFitur = [] for i in range ( len ( data [ 0 ])): meanFitur . append ( sum ( data [:, i ]) / len ( data )) return np . array ( meanFitur ) #untuk mencari rata rata per fitur def seleksiFiturPearson ( katadasar , data , threshold ): global meanFitur meanFitur = meanF ( data ) u = 0 while u len ( data [ 0 ]): dataBaru = data [:, : u + 1 ] meanBaru = meanFitur [: u + 1 ] katadasarBaru = katadasar [: u + 1 ] v = u while v len ( data [ 0 ]): if u != v : value = pearsonCalculate ( data , u , v ) if value threshold : dataBaru = np . hstack (( dataBaru , data [:, v ] . reshape ( data . shape [ 0 ], 1 ))) meanBaru = np . hstack (( meanBaru , meanFitur [ v ])) katadasarBaru = np . hstack (( katadasarBaru , katadasar [ v ])) v += 1 data = dataBaru meanFitur = meanBaru katadasar = katadasarBaru if u % 50 == 0 : print ( proses : , u , data . shape ) u += 1 return katadasar , data #untuk menseleksi fitur katadasarBaru , fiturBaru = seleksiFiturPearson ( katadasar , tfidf , 0.8 ) #untuk memanggil function for i in range ( 2 , len ( fiturBaru ) - 1 ): kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ s_avg = silhouette_score ( fiturBaru , classnya , random_state = 0 ) print ( Silhouette untuk , i , cluster adalah , s_avg ) print ( kmeans . labels_ ) #untuk mencari kmeans dan silhoutte print ( proses selesai ) with open ( Anggota_cluster.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) for i in classnya . reshape ( - 1 , 1 ): employee_writer . writerow ( i ) with open ( Seleksi_Fitur.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ([ katadasarBaru . tolist ()]) for i in fiturBaru : employee_writer . writerow ( i ) # untuk menyimpan data ke CSV","title":"Code dan Penjelasan"},{"location":"Seleksi_Fitur/#referensi","text":"https://wahyudisetiawan.wordpress.com/tag/seleksi-fitur/ http://www.softscients.web.id/2019/03/feature-selection-untuk-meningkatkan.html","title":"Referensi"},{"location":"Seleksi_Fitur/#hasil-running","text":"","title":"Hasil Running"},{"location":"Silhouette/","text":"Silhouette Silhouette berguna untuk mengetahui apakah suatu objek berada di dalam cluster yang tepat. for i in range ( 2 , len ( fiturBaru ) - 1 ): kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ s_avg = silhouette_score ( fiturBaru , classnya , random_state = 0 ) #digunakan untuk mengeksekusi nilai kmeans dengan silhouette agar menghasilkan nilai terbaik print ( Silhouette untuk , i , cluster adalah , s_avg ) print ( kmeans . labels_ ) #agar outputnya sesuai dengan kmeans dan nilai silhoutte agar user tindak kebingungan dalam membacanya Referensi http://jurnal.untan.ac.id/index.php/justin/article/viewFile/13119/11875 Hasil Output Program","title":"Evaluasi"},{"location":"Silhouette/#silhouette","text":"Silhouette berguna untuk mengetahui apakah suatu objek berada di dalam cluster yang tepat. for i in range ( 2 , len ( fiturBaru ) - 1 ): kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ s_avg = silhouette_score ( fiturBaru , classnya , random_state = 0 ) #digunakan untuk mengeksekusi nilai kmeans dengan silhouette agar menghasilkan nilai terbaik print ( Silhouette untuk , i , cluster adalah , s_avg ) print ( kmeans . labels_ ) #agar outputnya sesuai dengan kmeans dan nilai silhoutte agar user tindak kebingungan dalam membacanya","title":"Silhouette"},{"location":"Silhouette/#referensi","text":"http://jurnal.untan.ac.id/index.php/justin/article/viewFile/13119/11875","title":"Referensi"},{"location":"Silhouette/#hasil-output-program","text":"","title":"Hasil Output Program"},{"location":"TF-IDF/","text":"TF-IDF TF atau term frequency adalah weighting scheme yang digunakan untuk menentukan relevansi dokumen dengan sebuah query (term). DF adalah dengan mengurangi bobot TF suatu term dengan membaginya dengan frekuensi term terhadap koleksi dokumen (DF). Jadi sebuah term yang memiliki bobot TF yang besar namun dengan bobot DF yang besar pula tidak akan memiliki pengaruh yang besar dalam menentukan sebuah relevansi. Code dan Penjelasan Code tf-idf df = list () for d in range ( len ( matrix [ 0 ])): total = 0 for i in range ( len ( matrix )): if matrix [ i ][ d ] != 0 : total += 1 df . append ( total ) #digunakan untuk menghitung kata yang sama dalam keseluruhan hasil yang telah di crawling misal kata Rumah maka program akan mengeksekusi ada berapa banyak kata Rumah dari hasil yang telah di crawling idf = list () for i in df : tmp = 1 + log10 ( len ( matrix ) / ( 1 + i )) idf . append ( tmp ) #digunakan untuk menghitung hasil dari ada berapa banyak kata Rumah yang telah didapatkan dimasukan dalam rumus tf = matrix tfidf = [] for baris in range ( len ( matrix )): tampungBaris = [] for kolom in range ( len ( matrix [ 0 ])): tmp = tf [ baris ][ kolom ] * idf [ kolom ] tampungBaris . append ( tmp ) tfidf . append ( tampungBaris ) tfidf = np . array ( tfidf ) #hasil dari proses IDF diatas dikasikan dengan TF print ( tfidf ) print ( tfidf ) with open ( tf-idf.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in tfidf : employee_writer . writerow ( i ) Referensi https://prasetiautamacv.wordpress.com/2016/07/31/tf-idf-vsm-menggunakan-python/ Hasil Running","title":"Tf_Idf"},{"location":"TF-IDF/#tf-idf","text":"TF atau term frequency adalah weighting scheme yang digunakan untuk menentukan relevansi dokumen dengan sebuah query (term). DF adalah dengan mengurangi bobot TF suatu term dengan membaginya dengan frekuensi term terhadap koleksi dokumen (DF). Jadi sebuah term yang memiliki bobot TF yang besar namun dengan bobot DF yang besar pula tidak akan memiliki pengaruh yang besar dalam menentukan sebuah relevansi.","title":"TF-IDF"},{"location":"TF-IDF/#code-dan-penjelasan-code-tf-idf","text":"df = list () for d in range ( len ( matrix [ 0 ])): total = 0 for i in range ( len ( matrix )): if matrix [ i ][ d ] != 0 : total += 1 df . append ( total ) #digunakan untuk menghitung kata yang sama dalam keseluruhan hasil yang telah di crawling misal kata Rumah maka program akan mengeksekusi ada berapa banyak kata Rumah dari hasil yang telah di crawling idf = list () for i in df : tmp = 1 + log10 ( len ( matrix ) / ( 1 + i )) idf . append ( tmp ) #digunakan untuk menghitung hasil dari ada berapa banyak kata Rumah yang telah didapatkan dimasukan dalam rumus tf = matrix tfidf = [] for baris in range ( len ( matrix )): tampungBaris = [] for kolom in range ( len ( matrix [ 0 ])): tmp = tf [ baris ][ kolom ] * idf [ kolom ] tampungBaris . append ( tmp ) tfidf . append ( tampungBaris ) tfidf = np . array ( tfidf ) #hasil dari proses IDF diatas dikasikan dengan TF print ( tfidf ) print ( tfidf ) with open ( tf-idf.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in tfidf : employee_writer . writerow ( i )","title":"Code dan Penjelasan Code tf-idf"},{"location":"TF-IDF/#referensi","text":"https://prasetiautamacv.wordpress.com/2016/07/31/tf-idf-vsm-menggunakan-python/","title":"Referensi"},{"location":"TF-IDF/#hasil-running","text":"","title":"Hasil Running"},{"location":"VSM/","text":"VSM source code untuk VSM Vector Space Model (VSM) adalah suatu aljabar yang merepresentasikan kumpulan dokumen sebagai vetctor. VSM dapat diaplikasikan dalam klasifikasi dokumen, clustering dokumen, dan scoring dokumen terhadap sebuah query. Dalam VSM setiap dokumen direpresentasikan sebagai sebuah vector, dimana nilai dari setiap nilai dari vector tersebut mewakili weight sebuah term. Code dan Penjelasan factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () #digunakan untuk memeriksa kata apakah ada yang sama atau tidak dalam per-page yang telah melewati proses sastrawi factory = StemmerFactory () stemmer = factory . create_stemmer () tmp = for i in isi : tmp = tmp + + i hasil = [] for i in tmp . split (): try : if i . isalpha () and ( not i in hasil ) and len ( i ) 1 : # Menghilangkan Kata tidak penting stop = stopword . remove ( i ) if stop != : stem = stemmer . stem ( stop ) hasil . append ( stem ) except : continue katadasar = np . array ( hasil ) print ( vsm ) Menyeleksi hasil sastrawi dengan KBBI agar kata dasar lebih akurat #KBBI koneksi = sqlite3 . connect ( KBI.db ) cur_kbi = koneksi . execute ( SELECT* from KATA ) def LinearSearch ( kbi , kata ): found = False posisi = 0 while posisi len ( kata ) and not found : if kata [ posisi ] == kbi : found = True posisi = posisi + 1 return found berhasil = [] for kata in cur_kbi : ketemu = LinearSearch ( kata [ 0 ], katadasar ) if ketemu : kata = kata [ 0 ] berhasil . append ( kata ) print ( berhasil ) katadasar = np . array ( berhasil ) #digunakan untuk menyeleksi kembali hasil dari proses sastrawi agar lebih akurat kata dasar yang sesuai dalam KBBI matrix = [] for row in isi : tamp_isi = [] for a in katadasar : tamp_isi . append ( row . lower () . count ( a )) matrix . append ( tamp_isi ) print ( vsm ) #digunakan agar tatanan outputnya dalam bentuk matrix with open ( data_matrix.csv , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in matrix : employee_writer . writerow ( i ) #code untuk menyimpan hasil output dalam bentuk file CSV Penjelasan menambahkan code KBBI diperuntukkan menyeleksi kembali katakata yang telah diproses oleh sastrawi agar lebih akurat sesuai KBBI . Setelah selesai diproses maka hasil akan tertata dalam bentuk matrix yang akan di simpan dalam database dan CSV note : file databse KBBI harus berada dalam 1 folder file python yang akan dijalankan Referensi https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/ https://belajarpython.com/2018/05/sastrawi-natural-language-processing-bahasa-indonesia.html https://pypi.org/project/kbbi/ Hasil Running VSM Hasil Running Sastrawi Hasil Running KBBI , Hasil lebih sedikit dari hasil Sastrawi karena di seleksi lagi data yang lebih akurat menurut KBBI","title":"VSM"},{"location":"VSM/#vsm","text":"","title":"VSM"},{"location":"VSM/#source-code-untuk-vsm","text":"Vector Space Model (VSM) adalah suatu aljabar yang merepresentasikan kumpulan dokumen sebagai vetctor. VSM dapat diaplikasikan dalam klasifikasi dokumen, clustering dokumen, dan scoring dokumen terhadap sebuah query. Dalam VSM setiap dokumen direpresentasikan sebagai sebuah vector, dimana nilai dari setiap nilai dari vector tersebut mewakili weight sebuah term.","title":"source code untuk VSM"},{"location":"VSM/#code-dan-penjelasan","text":"factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () #digunakan untuk memeriksa kata apakah ada yang sama atau tidak dalam per-page yang telah melewati proses sastrawi factory = StemmerFactory () stemmer = factory . create_stemmer () tmp = for i in isi : tmp = tmp + + i hasil = [] for i in tmp . split (): try : if i . isalpha () and ( not i in hasil ) and len ( i ) 1 : # Menghilangkan Kata tidak penting stop = stopword . remove ( i ) if stop != : stem = stemmer . stem ( stop ) hasil . append ( stem ) except : continue katadasar = np . array ( hasil ) print ( vsm )","title":"Code dan Penjelasan"},{"location":"VSM/#menyeleksi-hasil-sastrawi-dengan-kbbi-agar-kata-dasar-lebih-akurat","text":"#KBBI koneksi = sqlite3 . connect ( KBI.db ) cur_kbi = koneksi . execute ( SELECT* from KATA ) def LinearSearch ( kbi , kata ): found = False posisi = 0 while posisi len ( kata ) and not found : if kata [ posisi ] == kbi : found = True posisi = posisi + 1 return found berhasil = [] for kata in cur_kbi : ketemu = LinearSearch ( kata [ 0 ], katadasar ) if ketemu : kata = kata [ 0 ] berhasil . append ( kata ) print ( berhasil ) katadasar = np . array ( berhasil ) #digunakan untuk menyeleksi kembali hasil dari proses sastrawi agar lebih akurat kata dasar yang sesuai dalam KBBI matrix = [] for row in isi : tamp_isi = [] for a in katadasar : tamp_isi . append ( row . lower () . count ( a )) matrix . append ( tamp_isi ) print ( vsm ) #digunakan agar tatanan outputnya dalam bentuk matrix with open ( data_matrix.csv , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in matrix : employee_writer . writerow ( i ) #code untuk menyimpan hasil output dalam bentuk file CSV","title":"Menyeleksi hasil sastrawi dengan KBBI agar kata dasar lebih akurat"},{"location":"VSM/#penjelasan","text":"menambahkan code KBBI diperuntukkan menyeleksi kembali katakata yang telah diproses oleh sastrawi agar lebih akurat sesuai KBBI . Setelah selesai diproses maka hasil akan tertata dalam bentuk matrix yang akan di simpan dalam database dan CSV note : file databse KBBI harus berada dalam 1 folder file python yang akan dijalankan","title":"Penjelasan"},{"location":"VSM/#referensi","text":"https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/ https://belajarpython.com/2018/05/sastrawi-natural-language-processing-bahasa-indonesia.html https://pypi.org/project/kbbi/","title":"Referensi"},{"location":"VSM/#hasil-running-vsm","text":"","title":"Hasil Running VSM"},{"location":"VSM/#hasil-running-sastrawi","text":"","title":"Hasil Running Sastrawi"},{"location":"VSM/#hasil-running-kbbi-hasil-lebih-sedikit-dari-hasil-sastrawi-karena-di-seleksi-lagi-data-yang-lebih-akurat-menurut-kbbi","text":"","title":"Hasil Running KBBI , Hasil lebih sedikit dari hasil Sastrawi karena di seleksi lagi data yang lebih akurat menurut KBBI"},{"location":"crawlinglink/","text":"Crawl Link Web structure mining adalah menyajikan beberapa subtopik integral, seperti struktur grafik dan pencarian, serta kategorisasi konten dan teknik klasifikasi untuk menetapkan dasar yang kuat untuk mengeksplorasi, mengekstraksi, dan menganalisis data informasi Web. Disini saya menggunakan link http://wardahbeauty.com/ Code dan Penjelasan def simplifiedURL ( url ): if www. in url : ind = url . index ( www. ) + 4 url = http:// + url [ ind :] if url [ - 1 ] == / : url = url [: - 1 ] parts = url . split ( / ) url = for i in range ( 3 ): url += parts [ i ] + / return url Code diatas berfungsi untuk mengambil link yang sudah dijadikan sebagai target. Mengecek link yang akan di carwling dengan memastikan tidak ada link yang sama dan menambahkan https pada link def crawl ( url , max_deep , show = False , deep = 0 , done = []): global edgelist deep += 1 url = simplifiedURL ( url ) if not url in done : links = getAllLinks ( url ) done . append ( url ) if show : if deep == 1 : print ( url ) else : print ( | , end = ) for i in range ( deep - 1 ): print ( -- , end = ) print ( ( %d ) %s % ( len ( links ), url )) for link in links : link = simplifiedURL ( link ) edge = ( url , link ) if not edge in edgelist : edgelist . append ( edge ) if ( deep != max_deep ): crawl ( link , max_deep , show , deep , done ) code diatas melakukan crawling dengan menggunakan 4 parameter yaitu url,max_deep,show,deep. deep : mengecek kedalaman crawl show : menampilkan hasil crawl max_deep : batasan kedalaman saat proses crawl url : target alamat yang akan di crawling if not url in done pengecekan apakah ketika proses crawling sudah pernah di crawling atau tidak , apabila belum maka akan dimasukkan kedalam list if (deep != max_deep): pengecekan kedalaman ketika me crawl data def getAllLinks ( src ): try : page = requests . get ( src ) soup = BeautifulSoup ( page . content , html.parser ) tags = soup . findAll ( a ) links = [] for tag in tags : try : link = tag [ href ] if not link in links and http in link : links . append ( link ) except KeyError : pass return links except : return list () code diatas untuk mengambil semua link yang ada dalam link http://wardahbeauty.com/ langkah mengambil semua link mengambil html nya mengubah html nya ke objek BeautifulSoup mengambil semua tag yang ada root = http://wardahbeauty.com/ nodelist = [ root ] edgelist = [] code diatas untuk menuju target link yang akan di crawling dan membuat list edgelist dan nodelist =[root] untuk menyimpan data yang akan diproses. Hasil Running Program","title":"Crawling Link"},{"location":"crawlinglink/#crawl-link","text":"Web structure mining adalah menyajikan beberapa subtopik integral, seperti struktur grafik dan pencarian, serta kategorisasi konten dan teknik klasifikasi untuk menetapkan dasar yang kuat untuk mengeksplorasi, mengekstraksi, dan menganalisis data informasi Web. Disini saya menggunakan link http://wardahbeauty.com/","title":"Crawl Link"},{"location":"crawlinglink/#code-dan-penjelasan","text":"def simplifiedURL ( url ): if www. in url : ind = url . index ( www. ) + 4 url = http:// + url [ ind :] if url [ - 1 ] == / : url = url [: - 1 ] parts = url . split ( / ) url = for i in range ( 3 ): url += parts [ i ] + / return url Code diatas berfungsi untuk mengambil link yang sudah dijadikan sebagai target. Mengecek link yang akan di carwling dengan memastikan tidak ada link yang sama dan menambahkan https pada link def crawl ( url , max_deep , show = False , deep = 0 , done = []): global edgelist deep += 1 url = simplifiedURL ( url ) if not url in done : links = getAllLinks ( url ) done . append ( url ) if show : if deep == 1 : print ( url ) else : print ( | , end = ) for i in range ( deep - 1 ): print ( -- , end = ) print ( ( %d ) %s % ( len ( links ), url )) for link in links : link = simplifiedURL ( link ) edge = ( url , link ) if not edge in edgelist : edgelist . append ( edge ) if ( deep != max_deep ): crawl ( link , max_deep , show , deep , done ) code diatas melakukan crawling dengan menggunakan 4 parameter yaitu url,max_deep,show,deep. deep : mengecek kedalaman crawl show : menampilkan hasil crawl max_deep : batasan kedalaman saat proses crawl url : target alamat yang akan di crawling if not url in done pengecekan apakah ketika proses crawling sudah pernah di crawling atau tidak , apabila belum maka akan dimasukkan kedalam list if (deep != max_deep): pengecekan kedalaman ketika me crawl data def getAllLinks ( src ): try : page = requests . get ( src ) soup = BeautifulSoup ( page . content , html.parser ) tags = soup . findAll ( a ) links = [] for tag in tags : try : link = tag [ href ] if not link in links and http in link : links . append ( link ) except KeyError : pass return links except : return list () code diatas untuk mengambil semua link yang ada dalam link http://wardahbeauty.com/ langkah mengambil semua link mengambil html nya mengubah html nya ke objek BeautifulSoup mengambil semua tag yang ada root = http://wardahbeauty.com/ nodelist = [ root ] edgelist = [] code diatas untuk menuju target link yang akan di crawling dan membuat list edgelist dan nodelist =[root] untuk menyimpan data yang akan diproses.","title":"Code dan Penjelasan"},{"location":"crawlinglink/#hasil-running-program","text":"","title":"Hasil Running Program"},{"location":"indexlink/","text":"Nama : NADILA HIDAYANTI NIM : 160411100182 SEMESTER : 6 MATA KULIAH : PENCARIAN DAN PENAMBANGAN WEB Library yang digunakan : Pandas Request BeautifulSoup Network as nx Matplotlib.pyplot as plt Code import pandas as pd # For crawling purpose import requests from bs4 import BeautifulSoup # For Graoh purpose import networkx as nx import matplotlib.pyplot as plt Penjelasan Fungsi Library Pandas adalah sebuah librari berlisensi BSD dan open source yang menyediakan struktur data dan analisis data yang mudah digunakan dan berkinerja tinggi untuk bahasa pemrograman Python. Request Digunakan saat me-Crawl data pada web BeautifulSoup merupakan library yang terdapat di python untuk menarik data yang ada di HTML. ada banyak library yang dapat digunakan untuk menarik data pada web namun saya kali ini menggunakan BeautifulSoup. Network as nx digunakan untuk menampilkan graph dari hasil codingan yang dijalankan Matplotlib berfungsi untuk membuat grafik yang rapi dalam cara yang mudah. Note : Jika tidak ingin mendownload library secara manual satu persatu di Idle Python, saya rekomendasikan menggunakan Spyder yang telah tersedia banyak library tanpa harus mendowload satu persatu, untuk tipe sesuaikan dengan laptop masing masing. Referensi https://code.tutsplus.com/id/tutorials/introducing-pandas--cms-26514 https://code.tutsplus.com/id/tutorials/introducing-matplotlib--cms-26543 https://datascience.or.id/article/Mengenal-BeautifulSoup-Untuk-WebScraping-5a8fa6e6 https://plot.ly/python/network-graphs/ https://www.researchgate.net/profile/Adi_Purnama4/publication/324703299_Graph_Mining_Pendahuluan/links/5addef0a458515c60f5f7c45/Graph-Mining-Pendahuluan.pdf?origin=publication_detail https://strukturdata01.wordpress.com/2015/01/26/graph/","title":"Pengantar"},{"location":"indexlink/#nama-nadila-hidayanti","text":"","title":"Nama : NADILA HIDAYANTI"},{"location":"indexlink/#nim-160411100182","text":"","title":"NIM : 160411100182"},{"location":"indexlink/#semester-6","text":"","title":"SEMESTER : 6"},{"location":"indexlink/#mata-kuliah-pencarian-dan-penambangan-web","text":"","title":"MATA KULIAH : PENCARIAN DAN PENAMBANGAN WEB"},{"location":"indexlink/#library-yang-digunakan","text":"Pandas Request BeautifulSoup Network as nx Matplotlib.pyplot as plt","title":"Library yang digunakan :"},{"location":"indexlink/#code","text":"import pandas as pd # For crawling purpose import requests from bs4 import BeautifulSoup # For Graoh purpose import networkx as nx import matplotlib.pyplot as plt","title":"Code"},{"location":"indexlink/#penjelasan-fungsi-library","text":"Pandas adalah sebuah librari berlisensi BSD dan open source yang menyediakan struktur data dan analisis data yang mudah digunakan dan berkinerja tinggi untuk bahasa pemrograman Python. Request Digunakan saat me-Crawl data pada web BeautifulSoup merupakan library yang terdapat di python untuk menarik data yang ada di HTML. ada banyak library yang dapat digunakan untuk menarik data pada web namun saya kali ini menggunakan BeautifulSoup. Network as nx digunakan untuk menampilkan graph dari hasil codingan yang dijalankan Matplotlib berfungsi untuk membuat grafik yang rapi dalam cara yang mudah.","title":"Penjelasan Fungsi Library"},{"location":"indexlink/#note","text":"Jika tidak ingin mendownload library secara manual satu persatu di Idle Python, saya rekomendasikan menggunakan Spyder yang telah tersedia banyak library tanpa harus mendowload satu persatu, untuk tipe sesuaikan dengan laptop masing masing.","title":"Note :"},{"location":"indexlink/#referensi","text":"https://code.tutsplus.com/id/tutorials/introducing-pandas--cms-26514 https://code.tutsplus.com/id/tutorials/introducing-matplotlib--cms-26543 https://datascience.or.id/article/Mengenal-BeautifulSoup-Untuk-WebScraping-5a8fa6e6 https://plot.ly/python/network-graphs/ https://www.researchgate.net/profile/Adi_Purnama4/publication/324703299_Graph_Mining_Pendahuluan/links/5addef0a458515c60f5f7c45/Graph-Mining-Pendahuluan.pdf?origin=publication_detail https://strukturdata01.wordpress.com/2015/01/26/graph/","title":"Referensi"}]}